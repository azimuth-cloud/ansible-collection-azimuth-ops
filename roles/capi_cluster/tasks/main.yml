---

- block:

    # This should only run on upgrade and not on initial provision
    - name: Pause MachineHealthChecks prior to cluster upgrade
      block:
      # If this is a fresh provision or the seed VM has been upgraded
      # the CAPI cluster's kubeconfig won't be available at
      # ~/{{ capi_cluster_release_name }}-kubeconfig
      # so need to refetch it from k8s secret
      - name: Attempt to generate kubeconfig for existing cluster
        command: >-
          kubectl get secret {{ capi_cluster_release_name }}-kubeconfig
            --namespace {{ capi_cluster_release_namespace }}
            --output jsonpath='{.data.value}'
        changed_when: false
        register: capi_cluster_kubeconfig_cmd
        # If this is an initial provision then secret won't yet exist so
        # we can skip this and subsequent machine health check pause steps
        failed_when: >-
          capi_cluster_kubeconfig_cmd.rc != 0 and
          "secrets \"{{ capi_cluster_release_name }}-kubeconfig\" not found"
          not in capi_cluster_kubeconfig_cmd.stderr

      - name: Write kubeconfig file for cluster
        copy:
          content: "{{ capi_cluster_kubeconfig_cmd.stdout | b64decode }}"
          dest: "{{ capi_cluster_kubeconfig_path }}"
          mode: u=rw,g=,o=
        when: capi_cluster_kubeconfig_cmd.rc == 0

      - name: Ensure MachineHealthChecks are paused
        command: >-
          kubectl
          --kubeconfig {{ capi_cluster_kubeconfig_path }}
          annotate --all --all-namespaces
          machinehealthchecks.cluster.x-k8s.io
          cluster.x-k8s.io/paused=true
        register: capi_cluster_mhc_pause
        changed_when: false
        # Allow for the case where this is the initial or partial provision
        # so the cluster kubeconfig is not yet available or doesn't yet have
        # the CAPI CRDs registered
        when: capi_cluster_kubeconfig_cmd.rc == 0
        failed_when: >-
          capi_cluster_mhc_pause.rc != 0 and
          "the server doesn't have a resource type"
          not in capi_cluster_mhc_pause.stderr

    - name: Install or upgrade cluster
      kubernetes.core.helm:
        chart_ref: "{{ capi_cluster_chart_name }}"
        chart_repo_url: "{{ capi_cluster_chart_repo }}"
        chart_version: "{{ capi_cluster_chart_version }}"
        release_namespace: "{{ capi_cluster_release_namespace }}"
        release_name: "{{ capi_cluster_release_name }}"
        release_state: present
        release_values: "{{ capi_cluster_release_values }}"
        create_namespace: yes
      register: capi_cluster_helm_release

    # The CAPI and CAPO controllers take some time to react and update the Ready condition
    # However some kinds of update, e.g. only addons, do not affect the cluster conditions,
    # so we only allow up to two minutes for that to happen
    - name: Wait for cluster to become unready
      command: >-
        kubectl wait clusters.cluster.x-k8s.io/{{ capi_cluster_release_name }}
          --for=condition=Ready=false
          --namespace {{ capi_cluster_release_namespace }}
          --timeout 0s
      changed_when: false
      register: capi_cluster_not_ready
      until: capi_cluster_not_ready is succeeded
      retries: 12
      delay: 10
      when: capi_cluster_helm_release is changed
      ignore_errors: true

    - name: Wait for cluster to become ready
      command: >-
        kubectl wait clusters.cluster.x-k8s.io/{{ capi_cluster_release_name }}
          --for=condition=Ready
          --namespace {{ capi_cluster_release_namespace }}
          --timeout 0s
      changed_when: false
      register: capi_cluster_ready
      until: capi_cluster_ready is succeeded
      retries: 360
      delay: 10

    # Note that because we waited for the cluster to become ready, we know
    # the control plane machines are at the correct version
    - name: Wait for all nodes to be at the same version
      command: kubectl get machines --output json
      changed_when: false
      register: capi_cluster_machines_list
      until: >-
        (
          capi_cluster_machines_list.stdout |
            from_json |
            json_query('items') |
            map(attribute = "status.nodeInfo.kubeletVersion", default = "NA") |
            unique |
            list |
            length
        ) == 1
      retries: 360
      delay: 10

    - name: Wait for machine deployments to be running
      command: >-
        kubectl wait machinedeployments --all
          --for=jsonpath='{.status.phase}'=Running
          --namespace {{ capi_cluster_release_namespace }}
          --timeout=0
      changed_when: false
      register: capi_cluster_mds_running
      until: capi_cluster_mds_running is succeeded
      retries: 360
      delay: 10

    - name: Wait for addons to deploy
      command: >-
        kubectl wait {{ item }} --all
          --for=jsonpath='{.status.phase}'=Deployed
          --namespace {{ capi_cluster_release_namespace }}
          --selector capi.stackhpc.com/cluster={{ capi_cluster_release_name }}
          --timeout=0
      changed_when: false
      register: capi_cluster_addons_complete
      until: capi_cluster_addons_complete is succeeded
      retries: 360
      delay: 10
      loop:
        - manifests
        - helmreleases

    - name: Generate kubeconfig for cluster
      command: >-
        kubectl get secret {{ capi_cluster_release_name }}-kubeconfig
          --namespace {{ capi_cluster_release_namespace }}
          --output jsonpath='{.data.value}'
      changed_when: false
      register: capi_cluster_kubeconfig_cmd

    - name: Write kubeconfig file for cluster
      copy:
        content: "{{ capi_cluster_kubeconfig_cmd.stdout | b64decode }}"
        dest: "{{ capi_cluster_kubeconfig_path }}"
        mode: u=rw,g=,o=

    # NOTE(sd109): It's not entirely clear whether unpausing MHCs here
    # is sufficient or whether it would be better to wait until after
    # Zenith role has run since this will also involve some downtime
    # for Zenith-proxied tenant cluster API server connection.
    - name: Ensure MachineHealthChecks are unpaused
      command: >-
        kubectl
        --kubeconfig {{ capi_cluster_kubeconfig_path }}
        annotate --all --all-namespaces
        machinehealthchecks.cluster.x-k8s.io
        cluster.x-k8s.io/paused-
      register: capi_cluster_mhc_unpause
      changed_when: false
      # Allow for the case where this is the initial provision
      # meaning that CAPI CRDs are not yet registered
      failed_when: >-
        capi_cluster_mhc_unpause.rc != 0 and
        "the server doesn't have a resource type"
        not in capi_cluster_mhc_unpause.stderr

  when: capi_cluster_release_state == 'present'

- block:
    # Before deleting the cluster, update the annotation that indicates if volumes should be kept
    - name: Set volumes policy annotation
      command: >-
        kubectl annotate
          openstackclusters.infrastructure.cluster.x-k8s.io
          {{ capi_cluster_release_name }}
          janitor.capi.stackhpc.com/volumes-policy={{ capi_cluster_volumes_policy }}
          --overwrite
          --namespace {{ capi_cluster_release_namespace }}

    - name: Delete cluster
      kubernetes.core.helm:
        release_namespace: "{{ capi_cluster_release_namespace }}"
        release_name: "{{ capi_cluster_release_name }}"
        release_state: absent
      register: capi_cluster_helm_delete

    - name: Wait for cluster to be deleted
      command: >-
        kubectl wait clusters.cluster.x-k8s.io/{{ capi_cluster_release_name }}
          --for=delete
          --namespace {{ capi_cluster_release_namespace }}
          --timeout 0s
      register: capi_cluster_delete
      when: capi_cluster_helm_delete is changed
      changed_when: false
      # Allow for the case where we never got as far as installing the CAPI controllers
      failed_when: >-
        capi_cluster_delete.rc != 0 and
        "the server doesn't have a resource type" not in capi_cluster_delete.stderr
      until: capi_cluster_delete is succeeded
      # Wait up to 60 mins for the cluster to delete
      retries: 360
      delay: 10
  when: capi_cluster_release_state == 'absent'
