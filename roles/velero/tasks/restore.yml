
- name: Check status of target backup
  # This also acts as check that target backup exists
  ansible.builtin.command: >-
    kubectl get backup
    -n "{{ velero_release_namespace }}"
    "{{ velero_restore_backup_name }}"
    -o jsonpath='{.status.phase}'
  register: velero_target_backup_check

- name: Fail if target backup is incomplete and strict mode is enabled
  ansible.builtin.fail: 
    msg: "Aborting restore of backup in state '{{ velero_target_backup_check.stdout }}'"
  when: velero_restore_strict and velero_target_backup_check.stdout != "Completed"

- name: Warn if target backup has status 'PartiallyFailed'
  debug:
    msg: "WARNING: Target backup as status 'PartiallyFailed' - restore may be ineffective"
  when: velero_target_backup_check.stdout == "PartiallyFailed"

- name: Perform Velero restore
  include_tasks: restore_loop.yml
  loop: "{{ range(1, velero_max_restore_passes+1) | list }}"

# This step is adapted from Consul restore docs here
# https://developer.hashicorp.com/consul/tutorials/kubernetes-production/kubernetes-disaster-recovery#recovery-steps
- name: Perform rolling restart of Consul cluster
  ansible.builtin.shell:
    cmd: |
      set -xe
      NAMESPACE=azimuth
      NAME_PATTERN=consul-server
      NODE_COUNT=$(kubectl -n $NAMESPACE get pods | grep $NAME_PATTERN- | wc -l)
      for (( i = 0; i < $NODE_COUNT; i++ )); do
        kubectl -n $NAMESPACE exec $NAME_PATTERN-$i -- consul leave
        sleep 2
        kubectl -n $NAMESPACE rollout status statefulset/$NAME_PATTERN --watch
      done
    executable: /usr/bin/bash # Loop syntax doesn't work with /bin/sh  

# Trigger service re-sync after fixing Consul cluster state
- name: Restart Zenith sync process
  ansible.builtin.shell: |
    kubectl -n azimuth rollout restart deployment/zenith-server-sync
    sleep 10

# Annotate cluster resources triggers a new watch event and 
# causes the addon provider to re-adopt the restored addons
- name: Annotate helmrelease and manifest resources to trigger capi addon readoption
  ansible.builtin.command: >-
    kubectl annotate "{{ item }}"
    velero.io/restore-time={{ '%Y-%m-%d--T%H:%M:%S' | strftime(ansible_date_time.epoch) }}
    -A --all --overwrite
  loop:
  - helmrelease
  - manifests

# Without this step, deleting tenant CAPI clusters after a restore fails because the CAPI Janitor
# doesn't think it's service account has permission to watch openstackcluster resources.
- name: Restart CAPI janitor to pick up restored ServiceAccount permissions
  ansible.builtin.command: >-
    kubectl -n capi-janitor-system rollout restart deployment/cluster-api-janitor-openstack