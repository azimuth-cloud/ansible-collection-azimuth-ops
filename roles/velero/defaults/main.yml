---
# Disable by default so that existing deployments don't break when bucket isn't configured
# Can be overwritten in azimuth-config as needed
velero_enabled: false

#####
# Velero installation and setup config
#####

# Velero CLI archive URL
velero_cli_repo: https://github.com/vmware-tanzu/velero
velero_cli_version: v1.17.1
velero_cli_os: "{{ ansible_system | lower }}"
velero_cli_arch: "{{ 'amd64' if ansible_architecture == 'x86_64' else ansible_architecture }}"
velero_cli_archive_name: >-
  velero-{{ velero_cli_version }}-{{ velero_cli_os }}-{{ velero_cli_arch }}.tar.gz
velero_cli_archive_url: >-
  {{ velero_cli_repo }}/releases/download/{{ velero_cli_version }}/{{ velero_cli_archive_name }}
# The directory into which the Velero CLI archive should be unpacked
velero_cli_unpack_directory: "/opt/velero/{{ velero_cli_version }}"
# The directory into which the Velero CLI binary should be placed
velero_cli_bin_directory: /usr/local/bin

# The trust bundle to use with the S3 endpoint
velero_trust_bundle: "{{ system_trust_ca_bundle | default('') }}"

# The URL endpoint for the target object store
velero_s3_url: "{{ undef(hint='velero_s3_url is required') }}"

# The name of a pre-existing bucket in the target object store
velero_bucket_name: "{{ undef(hint='velero_bucket_name is required') }}"

# The name to use for the secret containing the object store credentials
velero_s3_creds_secret_name: velero-s3-config

# S3 credentials
velero_aws_access_key_id: "{{ undef(hint='velero_aws_access_key_id is required') }}"
velero_aws_secret_access_key: "{{ undef(hint='velero_aws_secret_access_key is required') }}"

# Kubernetes CSI Snapshot Controller config
velero_csi_snapshot_controller_chart_name: snapshot-controller
velero_csi_snapshot_controller_chart_repo: https://piraeus.io/helm-charts/
velero_csi_snapshot_controller_chart_version: 4.1.0
velero_csi_snapshot_controller_release_namespace: kube-system
velero_csi_snapshot_controller_release_name: csi-snapshot-controller
velero_csi_snapshot_controller_wait_timeout: 10m
velero_csi_snapshot_controller_release_defaults: {}
velero_csi_snapshot_controller_release_overrides: {}
velero_csi_snapshot_controller_release_values: >-
  {{-
    velero_csi_snapshot_controller_release_defaults |
      combine(velero_csi_snapshot_controller_release_overrides, recursive = True)
  }}

# The name of the volume snapshot class
velero_cinder_snapshot_class_name: cinder-csi-snapshot

# Velero plugin config
velero_s3_plugin_image_source: velero/velero-plugin-for-aws
velero_s3_plugin_image_version: v1.13.1

# The default backup storage location
# We disable checksums because older Ceph doesn't implement them properly
velero_default_backup_storage_location: >-
  {{-
    {
      "name": "default",
      "default": True,
      "provider": "aws",
      "bucket": velero_bucket_name,
      "credential": {
        "name": velero_s3_creds_secret_name,
        "key": "s3-creds",
      },
      "config": {
        "s3Url": velero_s3_url,
        "s3ForcePathStyle": True,
        "checksumAlgorithm": "",
      },
    } |
      combine(
        {"caCert": (velero_trust_bundle | b64encode)}
        if velero_trust_bundle
        else {}
      )
  }}

# Velero Helm chart config
velero_chart_name: velero
velero_chart_repo: https://vmware-tanzu.github.io/helm-charts
velero_chart_version: 11.2.0
velero_release_namespace: velero
velero_release_name: velero
velero_wait_timeout: 10m
velero_release_defaults:
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
    prometheusRule:
      enabled: true
      spec:
        - alert: VeleroBackupPartialFailures
          annotations:
            # Use raw tags to prevent Ansible trying to render it as a template
            message: >-
              {%- raw -%}
              Velero schedule '{{ $labels.schedule }}' has partially failed backups in the last 24 hours.
              {%- endraw -%}
          expr: |-
            sum(increase(velero_backup_partial_failure_total{schedule!=""}[24h])) by(schedule) > 0
          for: 15m
          labels:
            severity: warning

        - alert: VeleroBackupFailures
          annotations:
            # Use raw tags to prevent Ansible trying to render it as a template
            message: >-
              {%- raw -%}
              Velero schedule '{{ $labels.schedule }}' has failed backups in the last 24 hours.
              {%- endraw -%}
          expr: |-
            sum(increase(velero_backup_failure_total{schedule!=""}[24h])) by(schedule) > 0
          for: 15m
          labels:
            severity: warning
  configuration:
    features: EnableCSI
    backupStorageLocation:
      - "{{ velero_default_backup_storage_location }}"
    volumeSnapshotLocation: []
  initContainers:
    - name: velero-plugin-for-aws
      image: "{{ velero_s3_plugin_image_source }}:{{ velero_s3_plugin_image_version }}"
      imagePullPolicy: IfNotPresent
      volumeMounts:
        - mountPath: /target
          name: plugins
velero_release_overrides: {}
velero_release_values: >-
  {{-
    velero_release_defaults |
      combine(velero_release_overrides, recursive = True)
  }}

#####
# Velero backup config
#####

# Whether or not to enable the scheduled backups
# NOTE(mkjpryor)
# When set to false, the schedule objects are still created but in a paused state
# This allows ad-hoc backups to be created using the scheduled backup as a template
velero_backup_schedule_enabled: true

# The name of the backup schedule
velero_backup_schedule_name: default

# The schedule for backups, using cron syntax
# See https://en.wikipedia.org/wiki/Cron for format options
velero_backup_schedule: "{{ velero_backup_schedule_timings | default('0 0 * * *') }}"

# Time-to-live for backups
# See https://pkg.go.dev/time#ParseDuration for duration format options
velero_backup_ttl: "{{ velero_schedule_ttl | default('168h') }}"

# Configuration for the backup
#   NOTE(mkjpryor)
#   The only critical volume with persistent state is the Keycloak DB
#   Restoring the metrics and logging volumes is harder because they are managed by the
#   CAPI Helm charts so are created before we have a chance to enact a restore, and restoring
#   a volume that already exists is a no-op

#   List of namespaces to include/exclude in backups
#     Velero does not handle wildcards other than "*" on its own here
#     So in order to catch all the per-tenant namespaces where platform resources are created,
#     we must exclude the system namespaces that we don't want to back up
#     Essentially, the included namespaces should be az-*, keycloak-system and zenith-services
velero_backup_included_namespaces:
  - "*"
velero_backup_excluded_namespaces:
  - azimuth
  - calico-apiserver
  - calico-system
  - capi-addon-system
  - capi-janitor-system
  - capi-kubeadm-bootstrap-system
  - capi-kubeadm-control-plane-system
  - capi-system
  - capo-system
  - cert-manager
  - default
  - harbor
  - ingress-nginx
  - kube-node-lease
  - kube-public
  - kube-system
  - kubernetes-dashboard
  - monitoring-system
  - node-problem-detector
  - openstack-system
  - postgres-operator
  - tigera-operator
  - velero
#   List of cluster-scoped resources to include/exclude in backups
velero_backup_included_cluster_scoped_resources:
  - clustertypes.caas.azimuth.stackhpc.com
  - clustertemplates.azimuth.stackhpc.com
  - apptemplates.azimuth.stackhpc.com
  - persistentvolumes
velero_backup_excluded_cluster_scoped_resources: []
#   List of namespace-scoped resources to include/exclude in backups
velero_backup_included_namespace_scoped_resources:
  - "*"
velero_backup_excluded_namespace_scoped_resources:
  - events
#   The spec for the scheduled backup
velero_backup_schedule_spec_defaults:
  paused: "{{ not velero_backup_schedule_enabled }}"
  schedule: "{{ velero_backup_schedule }}"
  useOwnerReferencesInBackup: false
  template:
    includedNamespaces: "{{ velero_backup_included_namespaces }}"
    excludedNamespaces: "{{ velero_backup_excluded_namespaces }}"
    includedClusterScopedResources: "{{ velero_backup_included_cluster_scoped_resources }}"
    excludedClusterScopedResources: "{{ velero_backup_excluded_cluster_scoped_resources }}"
    includedNamespaceScopedResources: "{{ velero_backup_included_namespace_scoped_resources }}"
    excludedNamespaceScopedResources: "{{ velero_backup_excluded_namespace_scoped_resources }}"
    snapshotVolumes: true
    ttl: "{{ velero_backup_ttl }}"
velero_backup_schedule_spec_overrides: {}
velero_backup_schedule_spec: >-
  {{-
    velero_backup_schedule_spec_defaults |
      combine(velero_backup_schedule_spec_overrides, recursive = True)
  }}

# TODO: Configure alerts for failed backups
# See https://github.com/vmware-tanzu/helm-charts/blob/1e6a5b46a59d2dae8153fa8c0794bad84e1d63e1/charts/velero/values.yaml#L239

#####
# Velero restore config (applies to restore playbook)
#####

# Name of backup to use for restore process
# If not given, the most recent successful backup for the schedule is used
velero_restore_backup_name: "{{ undef(hint='velero_restore_backup_name is required') }}"

# Name of restore object to create
velero_restore_name: >-
  {{ velero_restore_backup_name }}-{{ "%Y%m%d%H%M%S" | strftime(ansible_date_time.epoch) }}

# Resources that require their status to be restored in order to function correctly
velero_restore_include_resource_status:
  - clusters.caas.azimuth.stackhpc.com
  - leases.scheduling.azimuth.stackhpc.com

velero_restore_spec_defaults:
  restoreStatus:
    includedResources: "{{ velero_restore_include_resource_status }}"
  includeClusterResources: true
  restorePVs: true
  existingResourcePolicy: update
  backupName: "{{ velero_restore_backup_name }}"
velero_restore_spec_overrides: {}
velero_restore_spec: >-
  {{-
    velero_restore_spec_defaults |
      combine(velero_restore_spec_overrides, recursive = True)
  }}

# If true, allow restores from partially failed backups
# If false (the default), only allow restores from backups that completed without error
velero_restore_allow_partial: "{{ not (velero_restore_strict | default(True)) }}"
