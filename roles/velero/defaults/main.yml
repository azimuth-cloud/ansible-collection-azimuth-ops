---

# Disable by default so that existing deployments don't break when bucket isn't configured
# Can be overwritten in azimuth-config as needed
velero_enabled: false

# Whether or not to set the ansible no_log parameter on tasks in the role involving credentials.
# This should only ever be set false for temporary debugging purposes.
velero_no_log: true

#####
# Velero installation and setup config
#####

# Velero CLI archive URL
velero_cli_repo: https://github.com/vmware-tanzu/velero
velero_cli_version: v1.14.0
velero_cli_os: "{{ ansible_system | lower }}"
velero_cli_arch: "{{ 'amd64' if ansible_architecture == 'x86_64' else ansible_architecture }}"
velero_cli_archive_name: >-
  velero-{{ velero_cli_version }}-{{ velero_cli_os }}-{{ velero_cli_arch }}.tar.gz
velero_cli_archive_url: >-
  {{ velero_cli_repo }}/releases/download/{{ velero_cli_version }}/{{ velero_cli_archive_name }}
# The directory into which the Velero CLI archive should be unpacked
velero_cli_unpack_directory: "/opt/velero/{{ velero_cli_version }}"
# The directory into which the Velero CLI binary should be placed
velero_cli_bin_directory: /usr/local/bin

# The URL endpoint for the target object store
velero_s3_url: "{{ undef(hint = 'velero_s3_url is required') }}"

# The name of a pre-existing bucket in the target object store
velero_bucket_name: "{{ undef(hint = 'velero_bucket_name is required') }}"

# The name to use for the secret containing the object store credentials
velero_s3_creds_secret_name: velero-s3-config

# S3 credentials
velero_aws_access_key_id: "{{ undef(hint = 'velero_aws_access_key_id is required') }}"
velero_aws_secret_access_key: "{{ undef(hint = 'velero_aws_secret_access_key is required') }}"

# Kubernetes CSI Snapshot Controller config
velero_csi_snapshot_controller_chart_name: snapshot-controller
velero_csi_snapshot_controller_chart_repo: https://piraeus.io/helm-charts/
velero_csi_snapshot_controller_chart_version: 3.0.5
velero_csi_snapshot_controller_release_namespace: kube-system
velero_csi_snapshot_controller_release_name: csi-snapshot-controller
velero_csi_snapshot_controller_wait_timeout: 10m
velero_csi_snapshot_controller_release_defaults: {}
velero_csi_snapshot_controller_release_overrides: {}
velero_csi_snapshot_controller_release_values: >-
  {{-
    velero_csi_snapshot_controller_release_defaults |
      combine(velero_csi_snapshot_controller_release_overrides, recursive = True)
  }}

# The name of the volume snapshot class
velero_cinder_snapshot_class_name: cinder-csi-snapshot

# Velero plugin config
velero_s3_plugin_image_source: velero/velero-plugin-for-aws
velero_s3_plugin_image_version: v1.10.0
velero_csi_plugin_image_source: velero/velero-plugin-for-csi
velero_csi_plugin_image_version: v0.7.1

# Velero Helm chart config
velero_chart_name: velero
velero_chart_repo: https://vmware-tanzu.github.io/helm-charts
velero_chart_version: 6.7.0
velero_release_namespace: velero
velero_release_name: velero
velero_wait_timeout: 10m
velero_release_defaults:
  configuration:
    features: EnableCSI
    backupStorageLocation:
      - name: default
        default: true
        provider: aws
        bucket: "{{ velero_bucket_name }}"
        credential:
          name: "{{ velero_s3_creds_secret_name }}"
          key: s3-creds
        config:
          s3Url: "{{ velero_s3_url }}"
          s3ForcePathStyle: true
          # Older Ceph doesn't implement checksums properly
          checksumAlgorithm: ""
    volumeSnapshotLocation: []
  initContainers:
    - name: velero-plugin-for-aws
      image: "{{ velero_s3_plugin_image_source }}:{{ velero_s3_plugin_image_version }}"
      imagePullPolicy: IfNotPresent
      volumeMounts:
        - mountPath: /target
          name: plugins
    - name: velero-plugin-for-csi
      image: "{{ velero_csi_plugin_image_source }}:{{ velero_csi_plugin_image_version }}"
      imagePullPolicy: IfNotPresent
      volumeMounts:
        - mountPath: /target
          name: plugins
velero_release_overrides: {}
velero_release_values: >-
  {{-
    velero_release_defaults |
      combine(velero_release_overrides, recursive = True)
  }}

#####
# Velero backup config
#####

# Whether or not to enable the scheduled backups
# NOTE(mkjpryor)
# When set to false, the schedule objects are still created but in a paused state
# This allows ad-hoc backups to be created using the schedule objects as a template
velero_backup_schedule_enabled: true

# The schedule for backups, using cron syntax
# See https://en.wikipedia.org/wiki/Cron for format options
velero_backup_schedule: "{{ velero_backup_schedule_timings | default('0 0 * * *') }}"

# Time-to-live for backups
# See https://pkg.go.dev/time#ParseDuration for duration format options
velero_backup_ttl: "{{ velero_schedule_ttl | default('168h') }}"

# Configuration for the volumes backup schedule
# NOTE(mkjpryor)
# The only critical volumes with persistent state are Consul and the Keycloak DB
# All other volumes are ephemeral (e.g. Harbor, metrics, logs)
#   The name of the schedule
velero_schedule_volumes_name: volumes
#   The timing for the schedule
velero_schedule_volumes_schedule: "{{ velero_backup_schedule }}"
#   The TTL for backups created by the schedule
velero_schedule_volumes_ttl: "{{ velero_backup_ttl }}"
#   List of namespaces to include in backups
velero_schedule_volumes_included_namespaces:
  - azimuth
  - keycloak-system
#   List of namespaces to exclude from backups
velero_schedule_volumes_excluded_namespaces: []
#   List of cluster-scoped resources to include in backups
velero_schedule_volumes_included_cluster_scoped_resources:
  - persistentvolumes
#   List of cluster-scoped resources to exclude from backups
velero_schedule_volumes_excluded_cluster_scoped_resources: []
#   List of namespace-scoped resources to include in backups
velero_schedule_volumes_included_namespace_scoped_resources:
  - persistentvolumeclaims
#   List of namespace-scoped resources to exclude from backups
velero_schedule_volumes_excluded_namespace_scoped_resources: []
#   List of label selectors for resources to include in backups
velero_schedule_volumes_label_selectors:
  - matchLabels:
      app: consul
  - matchLabels:
      postgres-operator.crunchydata.com/cluster: keycloak-db
#   The spec for the scheduled backup
velero_schedule_volumes_spec_defaults:
  paused: "{{ not velero_backup_schedule_enabled }}"
  schedule: "{{ velero_schedule_volumes_schedule }}"
  useOwnerReferencesInBackup: false
  template:
    includedNamespaces: "{{ velero_schedule_volumes_included_namespaces }}"
    excludedNamespaces: "{{ velero_schedule_volumes_excluded_namespaces }}"
    includedClusterScopedResources: "{{ velero_schedule_volumes_included_cluster_scoped_resources }}"
    excludedClusterScopedResources: "{{ velero_schedule_volumes_excluded_cluster_scoped_resources }}"
    includedNamespaceScopedResources: "{{ velero_schedule_volumes_included_namespace_scoped_resources }}"
    excludedNamespaceScopedResources: "{{ velero_schedule_volumes_excluded_namespace_scoped_resources }}"
    orLabelSelectors: "{{ velero_schedule_volumes_label_selectors }}"
    snapshotVolumes: true
    ttl: "{{ velero_schedule_volumes_ttl }}"
velero_schedule_volumes_spec_overrides: {}
velero_schedule_volumes_spec: >-
  {{-
    velero_schedule_volumes_spec_defaults |
      combine(velero_schedule_volumes_spec_overrides, recursive = True)
  }}

# Configuration for the platforms backup schedule
# NOTE(mkjpryor)
#   We back up all the objects in namespaces of the form az-* and zenith-services, as well
#   as all the CaaS cluster types and Kubernetes cluster and app templates
#   In an ideal world, we would express this using the following variables:
#     velero_schedule_platforms_included_namespaces: [az-*, zenith-services]
#     velero_schedule_platforms_excluded_namespaces: []
#   But this is not currently possible - see https://github.com/vmware-tanzu/velero/issues/1874
#   So we achieve the same effect in a more brittle way be excluding the known namespaces
#   The name of the schedule
velero_schedule_platforms_name: platforms
#   The timing for the schedule
velero_schedule_platforms_schedule: "{{ velero_backup_schedule }}"
#   The TTL for backups created by the schedule
velero_schedule_platforms_ttl: "{{ velero_backup_ttl }}"
#   List of namespaces to include in backups
velero_schedule_platforms_included_namespaces:
  - "*"
#   List of namespaces to exclude from backups
velero_schedule_platforms_excluded_namespaces:
  - kube-system
#   List of cluster-scoped resources to include in backups
velero_schedule_platforms_included_cluster_scoped_resources:
  - clustertypes.caas.azimuth.stackhpc.com
  - clustertemplates.azimuth.stackhpc.com
  - apptemplates.azimuth.stackhpc.com
#   List of cluster-scoped resources to exclude from backups
velero_schedule_platforms_excluded_cluster_scoped_resources: []
#   List of namespace-scoped resources to include in backups
velero_schedule_platforms_included_namespace_scoped_resources:
  - "*"
#   List of namespace-scoped resources to exclude from backups
velero_schedule_platforms_excluded_namespace_scoped_resources:
  - events
#   List of label selectors for resources to include in backups
velero_schedule_platforms_label_selectors: []
#   The spec for the scheduled backup
velero_schedule_platforms_spec_defaults:
  paused: "{{ not velero_backup_schedule_enabled }}"
  schedule: "{{ velero_schedule_platforms_schedule }}"
  useOwnerReferencesInBackup: false
  template:
    includedNamespaces: "{{ velero_schedule_platforms_included_namespaces }}"
    excludedNamespaces: "{{ velero_schedule_platforms_excluded_namespaces }}"
    includedClusterScopedResources: "{{ velero_schedule_platforms_included_cluster_scoped_resources }}"
    excludedClusterScopedResources: "{{ velero_schedule_platforms_excluded_cluster_scoped_resources }}"
    includedNamespaceScopedResources: "{{ velero_schedule_platforms_included_namespace_scoped_resources }}"
    excludedNamespaceScopedResources: "{{ velero_schedule_platforms_excluded_namespace_scoped_resources }}"
    orLabelSelectors: "{{ velero_schedule_platforms_label_selectors }}"
    snapshotVolumes: false
    ttl: "{{ velero_schedule_platforms_ttl }}"
velero_schedule_platforms_spec_overrides: {}
velero_schedule_platforms_spec: >-
  {{-
    velero_schedule_platforms_spec_defaults |
      combine(velero_schedule_platforms_spec_overrides, recursive = True)
  }}

# TODO: Configure alerts for failed backups
# See https://github.com/vmware-tanzu/helm-charts/blob/1e6a5b46a59d2dae8153fa8c0794bad84e1d63e1/charts/velero/values.yaml#L239

#####
# Velero restore config (applies to stackhpc.azimuth_ops.restore playbook)
#####

# Name of backup to use for restore process
# If not given, the most recent successful backup for the schedule is used
velero_restore_backup_name:

# Name of restore object to create
velero_restore_name: >-
  {{ velero_restore_backup_name | default(velero_backup_schedule_name, true) }}-{{ '%Y%m%d%H%M%S' | strftime(ansible_date_time.epoch) }}

# CaaS clusters have persistent state in their status, so restore them
velero_restore_include_resource_status:
  - clusters.caas.azimuth.stackhpc.com

velero_restore_spec_defaults: >-
  {{-
    {
      "restoreStatus": {
        "includedResources": velero_restore_include_resource_status,
      },
      "includeClusterResources": True,
      "restorePVs": True,
      "existingResourcePolicy": "update",
    } |
      combine(
        {"backupName": velero_restore_backup_name}
        if velero_restore_backup_name
        else (
          {"scheduleName": velero_backup_schedule_name}
          if velero_backup_schedule_enabled
          else undef(hint = 'unable to determine backup to restore from')
        )
      )  
  }}
velero_restore_spec_overrides: {}
velero_restore_spec: >-
  {{-
    velero_backup_schedule_spec_defaults |
      combine(velero_backup_schedule_spec_overrides, recursive = True)
  }}

# Maximum number of restore passes to attempt
velero_max_restore_passes: 3

# Timeout in seconds for each restore pass
velero_restore_attempt_timeout: 1800

# Only allow restores from backups with status 'Completed'
# and bail if status is 'PartiallyFailed' (or anything else)
velero_restore_strict: true
